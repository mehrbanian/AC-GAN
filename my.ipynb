{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from skimage.io import imshow\n",
    "from tensorflow.keras.layers import ReLU, Dense, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose,\\\n",
    "                                    Input, concatenate, Conv2D, Flatten, Dropout\n",
    "from tensorflow.keras import Model\n",
    "from IPython.display import display\n",
    "import PIL\n",
    "from google.colab import files\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "def img_to_float(img):\n",
    "    return (np.float32(img) - 127.5) / 127.5\n",
    "\n",
    "def img_to_uint8(img):\n",
    "    return np.uint8(img * 127.5 + 128).clip(0, 255)\n",
    "\n",
    "\n",
    "train_img_f32 = img_to_float(train_images)\n",
    "BUFFER_SIZE = train_img_f32.shape[0]\n",
    "BATCH_SIZE = 100\n",
    "train_dataset_y = tf.data.Dataset.from_tensor_slices(train_labels[:, 0]).map(lambda y: tf.one_hot(y, 10))\n",
    "train_dataset_x = tf.data.Dataset.from_tensor_slices(train_img_f32)\n",
    "train_dataset = tf.data.Dataset.zip((train_dataset_x, train_dataset_y)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "latent_dim = 100\n",
    "\n",
    "def build_generator():\n",
    "    input_latent = Input((latent_dim,))\n",
    "    input_labels = Input((10,))\n",
    "    generator_input = concatenate([input_latent, input_labels])\n",
    "\n",
    "    gen_model = Dense(4 * 4 * 256, use_bias=False)(generator_input)\n",
    "    gen_model = ReLU()(gen_model)\n",
    "    gen_model = Reshape((4, 4, 256))(gen_model)\n",
    "    gen_model = Conv2DTranspose(192, kernel_size=(5, 5), strides=(2, 2), padding='same', use_bias=False)(gen_model)\n",
    "    gen_model = BatchNormalization()(gen_model)\n",
    "    gen_model = ReLU()(gen_model)\n",
    "    gen_model = Conv2DTranspose(96, kernel_size=(5, 5), strides=(2, 2), padding='same', use_bias=False)(gen_model)\n",
    "    gen_model = BatchNormalization()(gen_model)\n",
    "    gen_model = ReLU()(gen_model)\n",
    "    gen_model = Conv2DTranspose(3, kernel_size=(5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(gen_model)\n",
    "    # gen_model = BatchNormalization()(gen_model)\n",
    "    # gen_model = ReLU()(gen_model)\n",
    "    # gen_model = Conv2D(3, (3, 3), strides=(1, 1), padding='same', activation='tanh')(gen_model)\n",
    "\n",
    "    generator = Model(inputs=[input_latent, input_labels], outputs=gen_model)\n",
    "    return generator\n",
    "\n",
    "def build_discriminator():\n",
    "    discriminator_input = Input((32, 32, 3))\n",
    "    disc_model = Conv2D(16, kernel_size=(3, 3), strides=(2, 2), padding='same')(discriminator_input)\n",
    "    disc_model = LeakyReLU(alpha=0.2)(disc_model)\n",
    "    disc_model = Dropout(0.5)(disc_model)\n",
    "\n",
    "    disc_model = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False)(disc_model)\n",
    "    disc_model = BatchNormalization()(disc_model)\n",
    "    disc_model = LeakyReLU(alpha=0.2)(disc_model)\n",
    "    disc_model = Dropout(0.5)(disc_model)\n",
    "\n",
    "    disc_model = Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', use_bias=False)(disc_model)\n",
    "    disc_model = BatchNormalization()(disc_model)\n",
    "    disc_model = LeakyReLU(alpha=0.2)(disc_model)\n",
    "    disc_model = Dropout(0.5)(disc_model)\n",
    "\n",
    "    disc_model = Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False)(disc_model)\n",
    "    disc_model = BatchNormalization()(disc_model)\n",
    "    disc_model = LeakyReLU(alpha=0.2)(disc_model)\n",
    "    disc_model = Dropout(0.5)(disc_model)\n",
    "    \n",
    "    disc_model = Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', use_bias=False)(disc_model)\n",
    "    disc_model = BatchNormalization()(disc_model)\n",
    "    disc_model = LeakyReLU(alpha=0.2)(disc_model)\n",
    "    disc_model = Dropout(0.5)(disc_model)\n",
    "    \n",
    "    disc_model = Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False)(disc_model)\n",
    "    disc_model = BatchNormalization()(disc_model)\n",
    "    disc_model = LeakyReLU(alpha=0.2)(disc_model)\n",
    "    disc_model = Dropout(0.5)(disc_model)\n",
    "\n",
    "    disc_model = Flatten()(disc_model)\n",
    "    disc_output_d = Dense(1)(disc_model)\n",
    "    disc_output_c = Dense(10)(disc_model)\n",
    "\n",
    "    discriminator = Model(inputs=discriminator_input, outputs=[disc_output_d, disc_output_c])\n",
    "    return discriminator\n",
    "\n",
    "def generator_loss(generated_output, labels):\n",
    "    out_d, out_c = generated_output\n",
    "    loss_d = tf.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(out_d), out_d)\n",
    "    loss_c = tf.losses.CategoricalCrossentropy(from_logits=True)(labels, out_c)\n",
    "    return loss_d + loss_c\n",
    "\n",
    "def discriminator_loss(real_output, generated_output, labels):\n",
    "    real_out_d, real_out_c = real_output\n",
    "    real_loss_d = tf.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_out_d), real_out_d)\n",
    "    real_loss_c = tf.losses.CategoricalCrossentropy(from_logits=True)(labels, real_out_c)\n",
    "    real_loss = real_loss_d + real_loss_c\n",
    "\n",
    "    generated_out_d, generated_out_c = generated_output\n",
    "    generated_loss = tf.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(generated_out_d), generated_out_d)\n",
    "    total_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "generator_optimizer = tf.optimizers.Adam(2e-4, beta_1=0.5, beta_2=0.999)\n",
    "discriminator_optimizer = tf.optimizers.Adam(2e-4, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "tf.autograph.set_verbosity(0, True)\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    noise = tf.random.normal([BATCH_SIZE, latent_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator([noise, labels], training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        generated_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(generated_output, labels)\n",
    "        disc_loss = discriminator_loss(real_output, generated_output, labels)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "EPOCHS = 450\n",
    "num_examples_to_generate = 20\n",
    "\n",
    "random_vector_for_generation = tf.random.normal([num_examples_to_generate, latent_dim])\n",
    "condition_vector_generation = tf.one_hot(list(range(10)) + list(range(10)), 10)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    for images, labels in train_dataset:\n",
    "        train_step(images, labels)\n",
    "    fake = generator([random_vector_for_generation, condition_vector_generation], training=False)\n",
    "    fake_concat = np.transpose(img_to_uint8(fake), [1, 0, 2, 3]).reshape((32, -1, 3))\n",
    "    print(epoch, np.round(time.time() - start_time, 2))\n",
    "    plt.imshow(fake_concat)\n",
    "    plt.title(f'Fake images, Epoch: {epoch}', fontsize=9, fontweight='bold'), plt.axis('off')\n",
    "    plt.show()\n",
    "    # display(PIL.Image.fromarray(fake_concat))\n",
    "\n",
    "with open('generator_model.pickle', 'wb') as file:\n",
    "    pickle.dump(generator, file)\n",
    "\n",
    "with open('discriminator_model.pickle', 'wb') as file:\n",
    "    pickle.dump(discriminator, file)\n",
    "\n",
    "# Download the generator model file\n",
    "files.download('generator_model.pickle')\n",
    "\n",
    "# Download the discriminator model file\n",
    "files.download('discriminator_model.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffc2c986650f75bb84df5ef0f5794d173c138677d61245fd2c4ff2debf2f2371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
