{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_data():\n",
    "    # Specify the file paths for the CIFAR-10 dataset\n",
    "    data_dir = \"cifar-10\"\n",
    "    train_files = [\"data_batch_1\", \"data_batch_2\", \"data_batch_3\", \"data_batch_4\", \"data_batch_5\"]\n",
    "    test_file = \"test_batch\"\n",
    "    meta_file = \"batches.meta\"\n",
    "\n",
    "    # Function to load a single CIFAR-10 batch file\n",
    "    def load_cifar_batch(file_path):\n",
    "        with open(file_path, 'rb') as fo:\n",
    "            batch_dict = pickle.load(fo, encoding='bytes')\n",
    "        images = batch_dict[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        labels = np.array(batch_dict[b'labels'])\n",
    "        return images, labels\n",
    "\n",
    "    # Load training data\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    for train_file in train_files:\n",
    "        file_path = f\"{data_dir}/{train_file}\"\n",
    "        images, labels = load_cifar_batch(file_path)\n",
    "        train_images.append(images)\n",
    "        train_labels.append(labels)\n",
    "\n",
    "    train_images = np.concatenate(train_images)\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "\n",
    "    # Load test data\n",
    "    test_file_path = f\"{data_dir}/{test_file}\"\n",
    "    test_images, test_labels = load_cifar_batch(test_file_path)\n",
    "\n",
    "    # Load label names\n",
    "    meta_file_path = f\"{data_dir}/{meta_file}\"\n",
    "    with open(meta_file_path, 'rb') as fo:\n",
    "        label_names = pickle.load(fo, encoding='bytes')[b'label_names']\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_50 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_15 (Embedding)       (None, 1, 3072)      30720       ['input_50[0][0]']               \n",
      "                                                                                                  \n",
      " input_49 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " reshape_16 (Reshape)           (None, 32, 32, 3)    0           ['embedding_15[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 6)    0           ['input_49[0][0]',               \n",
      "                                                                  'reshape_16[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 64)   3520        ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, 16, 16, 64)   0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 8, 8, 128)    73856       ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_46 (LeakyReLU)     (None, 8, 8, 128)    0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 8192)         0           ['leaky_re_lu_46[0][0]']         \n",
      "                                                                                                  \n",
      " dense_44 (Dense)               (None, 256)          2097408     ['flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_47 (LeakyReLU)     (None, 256)          0           ['dense_44[0][0]']               \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 1)            257         ['leaky_re_lu_47[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,205,761\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,205,761\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_52 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_51 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_16 (Embedding)       (None, 1, 100)       1000        ['input_52[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_8 (Multiply)          (None, 1, 100)       0           ['input_51[0][0]',               \n",
      "                                                                  'embedding_16[0][0]']           \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 1, 256)       25856       ['multiply_8[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)     (None, 1, 256)       0           ['dense_46[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 1, 256)      1024        ['leaky_re_lu_48[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_47 (Dense)               (None, 1, 512)       131584      ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 1, 512)       0           ['dense_47[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 1, 512)      2048        ['leaky_re_lu_49[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, 1, 1024)      525312      ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_50 (LeakyReLU)     (None, 1, 1024)      0           ['dense_48[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 1, 1024)     4096        ['leaky_re_lu_50[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 1, 3072)      3148800     ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " reshape_17 (Reshape)           (None, 32, 32, 3)    0           ['dense_49[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,839,720\n",
      "Trainable params: 3,836,136\n",
      "Non-trainable params: 3,584\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 279ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='input_50'), name='input_50', description=\"created by layer 'input_50'\"), but it was called on an input with incompatible shape (32, 10).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"e:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"e:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"e:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"e:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'reshape_16' (type Reshape).\n    \n    Cannot reshape a tensor with 983040 elements to shape [32,32,32,3] (98304 elements) for '{{node model_22/reshape_16/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](model_22/embedding_15/embedding_lookup/Identity_1, model_22/reshape_16/Reshape/shape)' with input shapes: [32,10,3072], [4] and with input tensors computed as partial shapes: input[1] = [32,32,32,3].\n    \n    Call arguments received by layer 'reshape_16' (type Reshape):\n      • inputs=tf.Tensor(shape=(32, 10, 3072), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m plt\u001b[39m.\u001b[39mimshow(img_to_uint8(generated_images[\u001b[39m0\u001b[39m]))\n\u001b[0;32m    101\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m--> 103\u001b[0m discriminator_loss_real \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39;49mtrain_on_batch([real_images, real_labels], np\u001b[39m.\u001b[39;49mones((batch_size, \u001b[39m1\u001b[39;49m)))\n\u001b[0;32m    104\u001b[0m discriminator_loss_fake \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39mtrain_on_batch([generated_images, generated_labels], np\u001b[39m.\u001b[39mzeros((batch_size, \u001b[39m1\u001b[39m)))\n\u001b[0;32m    105\u001b[0m discriminator_loss \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39madd(discriminator_loss_real, discriminator_loss_fake)\n",
      "File \u001b[1;32me:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2478\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2474\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2475\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2476\u001b[0m     )\n\u001b[0;32m   2477\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2478\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   2480\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2481\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32me:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file19f1k0qg.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1233\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1230\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[0;32m   1232\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1233\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1234\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1235\u001b[0m     outputs,\n\u001b[0;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1237\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1238\u001b[0m )\n\u001b[0;32m   1239\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32me:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1222\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1222\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1223\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32me:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1023\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[39m# Run forward pass.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m-> 1023\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1024\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[0;32m   1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[1;32me:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"e:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"e:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"e:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"e:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'reshape_16' (type Reshape).\n    \n    Cannot reshape a tensor with 983040 elements to shape [32,32,32,3] (98304 elements) for '{{node model_22/reshape_16/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](model_22/embedding_15/embedding_lookup/Identity_1, model_22/reshape_16/Reshape/shape)' with input shapes: [32,10,3072], [4] and with input tensors computed as partial shapes: input[1] = [32,32,32,3].\n    \n    Call arguments received by layer 'reshape_16' (type Reshape):\n      • inputs=tf.Tensor(shape=(32, 10, 3072), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# AC-GAN generator\n",
    "def build_generator(latent_dim, num_classes):\n",
    "    generator_input = layers.Input(shape=(latent_dim,))\n",
    "    label_input = layers.Input(shape=(1,))\n",
    "    label_embedding = layers.Embedding(num_classes, latent_dim)(label_input)\n",
    "    merged_input = layers.multiply([generator_input, label_embedding])\n",
    "\n",
    "    x = layers.Dense(256)(merged_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(3072, activation='tanh')(x)\n",
    "    x = layers.Reshape((32, 32, 3))(x)\n",
    "\n",
    "    generator = keras.models.Model([generator_input, label_input], x)\n",
    "    return generator\n",
    "\n",
    "# AC-GAN discriminator\n",
    "def build_discriminator(num_classes):\n",
    "    discriminator_input = layers.Input(shape=(32, 32, 3))\n",
    "    label_input = layers.Input(shape=(1,))\n",
    "    label_embedding = layers.Embedding(num_classes, 32 * 32 * 3)(label_input)\n",
    "    flat_embedding = layers.Reshape((32, 32, 3))(label_embedding)\n",
    "    merged_input = layers.concatenate([discriminator_input, flat_embedding])\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same')(merged_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    discriminator_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    discriminator = keras.models.Model([discriminator_input, label_input], discriminator_output)\n",
    "    discriminator.compile(optimizer=keras.optimizers.Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return discriminator\n",
    "\n",
    "# AC-GAN combined model\n",
    "def build_acgan(generator, discriminator):\n",
    "    generator_input = layers.Input(shape=(latent_dim,))\n",
    "    label_input = layers.Input(shape=(1,))\n",
    "    generated_image = generator([generator_input, label_input])\n",
    "    discriminator_output = discriminator([generated_image, label_input])\n",
    "\n",
    "    acgan = keras.models.Model([generator_input, label_input], discriminator_output)\n",
    "    acgan.compile(optimizer=keras.optimizers.Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return acgan\n",
    "\n",
    "# Training the AC-GAN\n",
    "latent_dim = 100\n",
    "num_classes = 10\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "# (x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "x_train = x_train.astype('float32') / 255\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(num_classes)\n",
    "discriminator.trainable = False\n",
    "discriminator.summary()\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(latent_dim, num_classes)\n",
    "generator.summary()\n",
    "\n",
    "# Build and compile the AC-GAN\n",
    "acgan = build_acgan(generator, discriminator)\n",
    "\n",
    "def img_to_uint8(img):\n",
    "    return np.uint8(img*127.5+128).clip(0, 255)\n",
    "\n",
    "# Training loop\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in range(len(x_train) // batch_size):\n",
    "        # Training the discriminator\n",
    "        random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "        generated_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
    "        generated_images = generator.predict([random_latent_vectors, generated_labels])\n",
    "\n",
    "        real_images = x_train[batch * batch_size: (batch + 1) * batch_size]\n",
    "        real_labels = y_train[batch * batch_size: (batch + 1) * batch_size]\n",
    "        plt.imshow(img_to_uint8(generated_images[0]))\n",
    "        plt.show()\n",
    "\n",
    "        discriminator_loss_real = discriminator.train_on_batch([real_images, real_labels], np.ones((batch_size, 1)))\n",
    "        discriminator_loss_fake = discriminator.train_on_batch([generated_images, generated_labels], np.zeros((batch_size, 1)))\n",
    "        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "        # Training the generator\n",
    "        random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "        generated_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
    "\n",
    "        acgan_loss = acgan.train_on_batch([random_latent_vectors, generated_labels], np.ones((batch_size, 1)))\n",
    "\n",
    "        print(f\"Batch {batch+1}/{len(x_train) // batch_size} | D loss: {discriminator_loss[0]:.4f}, G loss: {acgan_loss[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape_8\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [10, 8192], output_shape = [8, 8, 128]\n\nCall arguments received by layer \"reshape_8\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 10, 8192), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m latent_dim \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m     66\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m---> 68\u001b[0m generator \u001b[39m=\u001b[39m build_generator(latent_dim, num_classes)\n\u001b[0;32m     69\u001b[0m discriminator \u001b[39m=\u001b[39m build_discriminator(num_classes)\n\u001b[0;32m     70\u001b[0m acgan \u001b[39m=\u001b[39m build_acgan(generator, discriminator)\n",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36mbuild_generator\u001b[1;34m(latent_dim, num_classes)\u001b[0m\n\u001b[0;32m      5\u001b[0m merged_input \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mmultiply([generator_input, label_embedding])\n\u001b[0;32m      7\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDense(\u001b[39m128\u001b[39m \u001b[39m*\u001b[39m \u001b[39m8\u001b[39m \u001b[39m*\u001b[39m \u001b[39m8\u001b[39m)(merged_input)\n\u001b[1;32m----> 8\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mReshape((\u001b[39m8\u001b[39;49m, \u001b[39m8\u001b[39;49m, \u001b[39m128\u001b[39;49m))(x)\n\u001b[0;32m     10\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2DTranspose(\u001b[39m128\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m), strides\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m), padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(x)\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mLeakyReLU()(x)\n",
      "File \u001b[1;32me:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\layers\\reshaping\\reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[1;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[0;32m    116\u001b[0m     output_shape[unknown] \u001b[39m=\u001b[39m original \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m known\n\u001b[0;32m    117\u001b[0m \u001b[39melif\u001b[39;00m original \u001b[39m!=\u001b[39m known:\n\u001b[1;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m output_shape\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape_8\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [10, 8192], output_shape = [8, 8, 128]\n\nCall arguments received by layer \"reshape_8\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 10, 8192), dtype=float32)"
     ]
    }
   ],
   "source": [
    "def build_generator(latent_dim, num_classes):\n",
    "    generator_input = keras.Input(shape=(latent_dim,))\n",
    "    label_input = keras.Input(shape=(num_classes,))\n",
    "    label_embedding = layers.Embedding(num_classes, latent_dim)(label_input)\n",
    "    merged_input = layers.multiply([generator_input, label_embedding])\n",
    "\n",
    "    x = layers.Dense(128 * 8 * 8)(merged_input)\n",
    "    x = layers.Reshape((8, 8, 128))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(256, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(512, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(3, kernel_size=(3, 3), padding='same', activation='tanh')(x)\n",
    "\n",
    "    generator = keras.models.Model([generator_input, label_input], x)\n",
    "    return generator\n",
    "\n",
    "\n",
    "def build_discriminator(num_classes):\n",
    "    discriminator_input = keras.Input(shape=(32, 32, 3))\n",
    "    label_input = keras.Input(shape=(num_classes,))\n",
    "    label_embedding = layers.Embedding(num_classes, 32 * 32 * 3)(label_input)\n",
    "    flat_embedding = layers.Reshape((32, 32, 3))(label_embedding)\n",
    "    merged_input = layers.concatenate([discriminator_input, flat_embedding])\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same')(merged_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    discriminator_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    discriminator = keras.models.Model([discriminator_input, label_input], discriminator_output)\n",
    "    discriminator.compile(optimizer=keras.optimizers.Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return discriminator\n",
    "\n",
    "def build_acgan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    acgan_input = keras.Input(shape=(latent_dim,))\n",
    "    acgan_label = keras.Input(shape=(num_classes,))\n",
    "    generated_image = generator([acgan_input, acgan_label])\n",
    "    acgan_output = discriminator([generated_image, acgan_label])\n",
    "    \n",
    "    acgan = keras.models.Model([acgan_input, acgan_label], acgan_output)\n",
    "    acgan.compile(optimizer=keras.optimizers.Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return acgan\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "# (x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "x_train = x_train.astype('float32') / 255\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "latent_dim = 100\n",
    "num_classes = 10\n",
    "\n",
    "generator = build_generator(latent_dim, num_classes)\n",
    "discriminator = build_discriminator(num_classes)\n",
    "acgan = build_acgan(generator, discriminator)\n",
    "\n",
    "# Training the AC-GAN\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in range(len(x_train) // batch_size):\n",
    "        # Training the discriminator\n",
    "        random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "        generated_labels = np.random.randint(0, num_classes, batch_size)\n",
    "        generated_labels = keras.utils.to_categorical(generated_labels, num_classes)  # Convert to one-hot encoding\n",
    "        generated_images = generator.predict([random_latent_vectors, generated_labels])\n",
    "\n",
    "        real_images = x_train[batch * batch_size: (batch + 1) * batch_size]\n",
    "        real_labels = y_train[batch * batch_size: (batch + 1) * batch_size]\n",
    "        print(real_images.shape)\n",
    "\n",
    "        discriminator_labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "\n",
    "        discriminator_loss = discriminator.train_on_batch(\n",
    "            [np.concatenate([real_images, generated_images]), np.concatenate([real_labels, generated_labels])],\n",
    "            discriminator_labels\n",
    "        )\n",
    "\n",
    "        # Training the generator\n",
    "        random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "        generated_labels = np.random.randint(0, num_classes, batch_size)\n",
    "        generated_labels = keras.utils.to_categorical(generated_labels, num_classes)  # Convert to one-hot encoding\n",
    "\n",
    "        acgan_loss = acgan.train_on_batch([random_latent_vectors, generated_labels], np.ones((batch_size, 1)))\n",
    "\n",
    "        print(f\"Batch {batch+1}/{len(x_train) // batch_size} | D loss: {discriminator_loss[0]:.4f}, G loss: {acgan_loss[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_45 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_46 (InputLayer)          [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 110)          0           ['input_45[0][0]',               \n",
      "                                                                  'input_46[0][0]']               \n",
      "                                                                                                  \n",
      " dense_43 (Dense)               (None, 32768)        3637248     ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_42 (LeakyReLU)     (None, 32768)        0           ['dense_43[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_15 (Reshape)           (None, 16, 16, 128)  0           ['leaky_re_lu_42[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 32, 32, 128)  262272     ['reshape_15[0][0]']             \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " leaky_re_lu_43 (LeakyReLU)     (None, 32, 32, 128)  0           ['conv2d_transpose[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 256)  524544     ['leaky_re_lu_43[0][0]']         \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, 64, 64, 256)  0           ['conv2d_transpose_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 3)  12291      ['leaky_re_lu_44[0][0]']         \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,436,355\n",
      "Trainable params: 4,436,355\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 32, 32, 3), (None, 10)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m discriminator_inputs \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[0;32m     36\u001b[0m discriminator_labels \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,))\n\u001b[1;32m---> 37\u001b[0m discriminator_input \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mConcatenate()([discriminator_inputs, discriminator_labels])\n\u001b[0;32m     39\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(\u001b[39m64\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), strides\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m), padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msame\u001b[39m\u001b[39m\"\u001b[39m)(discriminator_input)\n\u001b[0;32m     40\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mLeakyReLU()(x)\n",
      "File \u001b[1;32me:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\Users\\AbbasMn\\anaconda3\\lib\\site-packages\\keras\\layers\\merging\\concatenate.py:119\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    117\u001b[0m ranks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mlen\u001b[39m(shape) \u001b[39mfor\u001b[39;00m shape \u001b[39min\u001b[39;00m shape_set)\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ranks) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err_msg)\n\u001b[0;32m    120\u001b[0m \u001b[39m# Get the only rank for the set.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m (rank,) \u001b[39m=\u001b[39m ranks\n",
      "\u001b[1;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 32, 32, 3), (None, 10)]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize input images\n",
    "x_train = (x_train - 127.5) / 127.5\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "\n",
    "# Generator\n",
    "latent_dim = 100\n",
    "\n",
    "generator_inputs = keras.Input(shape=(latent_dim,))\n",
    "generator_labels = keras.Input(shape=(10,))\n",
    "generator_input = layers.Concatenate()([generator_inputs, generator_labels])\n",
    "\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\")(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), padding=\"same\")(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(3, (4, 4), strides=(2, 2), padding=\"same\", activation=\"tanh\")(x)\n",
    "\n",
    "generator = keras.models.Model([generator_inputs, generator_labels], x)\n",
    "generator.summary()\n",
    "\n",
    "# Discriminator\n",
    "discriminator_inputs = keras.Input(shape=(32, 32, 3))\n",
    "discriminator_labels = keras.Input(shape=(10,))\n",
    "discriminator_input = layers.Concatenate()([discriminator_inputs, discriminator_labels])\n",
    "\n",
    "x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\")(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "x = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "discriminator = keras.models.Model([discriminator_inputs, discriminator_labels], x)\n",
    "discriminator.summary()\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics=[\"accuracy\"])\n",
    "\n",
    "# GAN\n",
    "discriminator.trainable = False\n",
    "gan_inputs = keras.Input(shape=(latent_dim,))\n",
    "gan_labels = keras.Input(shape=(10,))\n",
    "gan_output = discriminator([generator([gan_inputs, gan_labels]), gan_labels])\n",
    "\n",
    "gan = keras.models.Model([gan_inputs, gan_labels], gan_output)\n",
    "gan.summary()\n",
    "\n",
    "# Compile GAN\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "# Training loop\n",
    "epochs = 20000\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train discriminator\n",
    "    random_indices = np.random.randint(0, x_train.shape[0], size=batch_size)\n",
    "    real_images = x_train[random_indices]\n",
    "    real_labels = y_train[random_indices]\n",
    "\n",
    "    noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "    generated_images = generator.predict([noise, real_labels])\n",
    "\n",
    "    labels = np.concatenate([real_labels, real_labels])\n",
    "    images = np.concatenate([real_images, generated_images])\n",
    "    labels_noise = np.concatenate([real_labels, real_labels])\n",
    "    images_noise = np.concatenate([generated_images, generated_images])\n",
    "\n",
    "    labels_noise[np.arange(batch_size), np.random.randint(0, 10, size=batch_size)] = 1\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch([images, labels], np.ones((2 * batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch([images_noise, labels_noise], np.zeros((2 * batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train generator\n",
    "    noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "    labels = keras.utils.to_categorical(np.random.randint(0, 10, batch_size), num_classes=10)\n",
    "\n",
    "    g_loss = gan.train_on_batch([noise, labels], np.ones((batch_size, 1)))\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Discriminator Loss: {d_loss[0]:.4f} | Discriminator Accuracy: {d_loss[1]*100:.2f}% | Generator Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Generate images\n",
    "num_samples = 10\n",
    "noise = np.random.normal(0, 1, size=(num_samples, latent_dim))\n",
    "labels = keras.utils.to_categorical(np.arange(0, 10), num_classes=10)\n",
    "generated_images = generator.predict([noise, labels])\n",
    "\n",
    "# Denormalize images\n",
    "generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "# Display generated images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, num_samples, figsize=(num_samples, 1))\n",
    "fig.suptitle(\"Generated Images\")\n",
    "\n",
    "for i in range(num_samples):\n",
    "    axs[i].imshow(generated_images[i].astype(np.uint8))\n",
    "    axs[i].axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffc2c986650f75bb84df5ef0f5794d173c138677d61245fd2c4ff2debf2f2371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
