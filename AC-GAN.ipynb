{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_data():\n",
    "    # Specify the file paths for the CIFAR-10 dataset\n",
    "    data_dir = \"cifar-10\"\n",
    "    train_files = [\"data_batch_1\", \"data_batch_2\", \"data_batch_3\", \"data_batch_4\", \"data_batch_5\"]\n",
    "    test_file = \"test_batch\"\n",
    "    meta_file = \"batches.meta\"\n",
    "\n",
    "    # Function to load a single CIFAR-10 batch file\n",
    "    def load_cifar_batch(file_path):\n",
    "        with open(file_path, 'rb') as fo:\n",
    "            batch_dict = pickle.load(fo, encoding='bytes')\n",
    "        images = batch_dict[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        labels = np.array(batch_dict[b'labels'])\n",
    "        return images, labels\n",
    "\n",
    "    # Load training data\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    for train_file in train_files:\n",
    "        file_path = f\"{data_dir}/{train_file}\"\n",
    "        images, labels = load_cifar_batch(file_path)\n",
    "        train_images.append(images)\n",
    "        train_labels.append(labels)\n",
    "\n",
    "    train_images = np.concatenate(train_images)\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "\n",
    "    # Load test data\n",
    "    test_file_path = f\"{data_dir}/{test_file}\"\n",
    "    test_images, test_labels = load_cifar_batch(test_file_path)\n",
    "\n",
    "    # Load label names\n",
    "    meta_file_path = f\"{data_dir}/{meta_file}\"\n",
    "    with open(meta_file_path, 'rb') as fo:\n",
    "        label_names = pickle.load(fo, encoding='bytes')[b'label_names']\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"d:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'reshape_6' (type Reshape).\n    \n    Cannot reshape a tensor with 1966080 elements to shape [64,32,32,3] (196608 elements) for '{{node model_9/reshape_6/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](model_9/embedding_6/embedding_lookup/Identity_1, model_9/reshape_6/Reshape/shape)' with input shapes: [64,10,3072], [4] and with input tensors computed as partial shapes: input[1] = [64,32,32,3].\n    \n    Call arguments received by layer 'reshape_6' (type Reshape):\n      â€¢ inputs=tf.Tensor(shape=(64, 10, 3072), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m real_images \u001b[39m=\u001b[39m x_train[batch \u001b[39m*\u001b[39m batch_size: (batch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m batch_size]\n\u001b[0;32m     93\u001b[0m real_labels \u001b[39m=\u001b[39m y_train[batch \u001b[39m*\u001b[39m batch_size: (batch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m batch_size]\n\u001b[1;32m---> 95\u001b[0m discriminator_loss_real \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39;49mtrain_on_batch([real_images, real_labels], np\u001b[39m.\u001b[39;49mones((batch_size, \u001b[39m1\u001b[39;49m)))\n\u001b[0;32m     96\u001b[0m discriminator_loss_fake \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39mtrain_on_batch([generated_images, generated_labels], np\u001b[39m.\u001b[39mzeros((batch_size, \u001b[39m1\u001b[39m)))\n\u001b[0;32m     97\u001b[0m discriminator_loss \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39madd(discriminator_loss_real, discriminator_loss_fake)\n",
      "File \u001b[1;32md:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2510\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2506\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2507\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2508\u001b[0m     )\n\u001b[0;32m   2509\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2510\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   2512\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32md:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileo66zpajq.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1268\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1265\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m     )\n\u001b[0;32m   1267\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1268\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1269\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1270\u001b[0m     outputs,\n\u001b[0;32m   1271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1272\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1273\u001b[0m )\n\u001b[0;32m   1274\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32md:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1249\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1250\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32md:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1050\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# Run forward pass.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m-> 1050\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1051\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[0;32m   1052\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[1;32md:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"d:\\Users\\Abbas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'reshape_6' (type Reshape).\n    \n    Cannot reshape a tensor with 1966080 elements to shape [64,32,32,3] (196608 elements) for '{{node model_9/reshape_6/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](model_9/embedding_6/embedding_lookup/Identity_1, model_9/reshape_6/Reshape/shape)' with input shapes: [64,10,3072], [4] and with input tensors computed as partial shapes: input[1] = [64,32,32,3].\n    \n    Call arguments received by layer 'reshape_6' (type Reshape):\n      â€¢ inputs=tf.Tensor(shape=(64, 10, 3072), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# AC-GAN generator\n",
    "def build_generator(latent_dim, num_classes):\n",
    "    generator_input = layers.Input(shape=(latent_dim,))\n",
    "    label_input = layers.Input(shape=(1,))\n",
    "    label_embedding = layers.Embedding(num_classes, latent_dim)(label_input)\n",
    "    merged_input = layers.multiply([generator_input, label_embedding])\n",
    "\n",
    "    x = layers.Dense(256)(merged_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(3072, activation='tanh')(x)\n",
    "    x = layers.Reshape((32, 32, 3))(x)\n",
    "\n",
    "    generator = keras.models.Model([generator_input, label_input], x)\n",
    "    return generator\n",
    "\n",
    "# AC-GAN discriminator\n",
    "def build_discriminator(num_classes):\n",
    "    discriminator_input = layers.Input(shape=(32, 32, 3))\n",
    "    label_input = layers.Input(shape=(1,))\n",
    "    label_embedding = layers.Embedding(num_classes, 32 * 32 * 3)(label_input)\n",
    "    flat_embedding = layers.Reshape((32, 32, 3))(label_embedding)\n",
    "    merged_input = layers.concatenate([discriminator_input, flat_embedding])\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same')(merged_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    discriminator_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    discriminator = keras.models.Model([discriminator_input, label_input], discriminator_output)\n",
    "    discriminator.compile(optimizer=keras.optimizers.Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return discriminator\n",
    "\n",
    "# AC-GAN combined model\n",
    "def build_acgan(generator, discriminator):\n",
    "    generator_input = layers.Input(shape=(latent_dim,))\n",
    "    label_input = layers.Input(shape=(1,))\n",
    "    generated_image = generator([generator_input, label_input])\n",
    "    discriminator_output = discriminator([generated_image, label_input])\n",
    "\n",
    "    acgan = keras.models.Model([generator_input, label_input], discriminator_output)\n",
    "    acgan.compile(optimizer=keras.optimizers.Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return acgan\n",
    "\n",
    "# Training the AC-GAN\n",
    "latent_dim = 100\n",
    "num_classes = 10\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "x_train, y_train, _, _ = load_data()\n",
    "x_train = x_train.astype('float32') / 255\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(num_classes)\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(latent_dim, num_classes)\n",
    "\n",
    "# Build and compile the AC-GAN\n",
    "acgan = build_acgan(generator, discriminator)\n",
    "\n",
    "# Training loop\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in range(len(x_train) // batch_size):\n",
    "        # Training the discriminator\n",
    "        random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "        generated_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
    "        generated_images = generator.predict([random_latent_vectors, generated_labels])\n",
    "\n",
    "        real_images = x_train[batch * batch_size: (batch + 1) * batch_size]\n",
    "        real_labels = y_train[batch * batch_size: (batch + 1) * batch_size]\n",
    "\n",
    "        discriminator_loss_real = discriminator.train_on_batch([real_images, real_labels], np.ones((batch_size, 1)))\n",
    "        discriminator_loss_fake = discriminator.train_on_batch([generated_images, generated_labels], np.zeros((batch_size, 1)))\n",
    "        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "        # Training the generator\n",
    "        random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "        generated_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
    "\n",
    "        acgan_loss = acgan.train_on_batch([random_latent_vectors, generated_labels], np.ones((batch_size, 1)))\n",
    "\n",
    "        print(f\"Batch {batch+1}/{len(x_train) // batch_size} | D loss: {discriminator_loss[0]:.4f}, G loss: {acgan_loss[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
